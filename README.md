# A DIVE INTO THE PERFORMANCE ANALYSIS OF THE BERT FAMILY AGAINST THE FNC-1 TASK
Transformer based models have led to a revolution in NLP, as they
can reach SOTA in a wide variety of tasks, and for this reason many
people associate their introduction into NLP with “ImageNet moment”
in Computer Vision. In this project I try to test many different kinds
of Transformer encoder architecture, which belongs to the BERT family, considering both transfer learning techniques through fine tuning of
pre-trained model and feature extraction to obtain contextual word embedding. All my trials were done to solve the FNC-1 challenge and led
me to improve the SOTA results reported in the challenge website.

The full documentation can be found [here](Report.pdf)
